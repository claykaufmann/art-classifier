{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to have the data locally locally saved as `/data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save the data directories into variables\n",
    "\"\"\"\n",
    "\n",
    "main_direc = os.getcwd()\n",
    "images_dir = os.path.join(main_direc, 'data/images/images')\n",
    "\n",
    "# csv location\n",
    "artist_csv_loc = os.path.join(main_direc, 'data/artists.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Set hyperparams for images\n",
    "\"\"\"\n",
    "\n",
    "IMG_WIDTH = 100\n",
    "IMG_HEIGHT = 100\n",
    "BATCH_SIZE = 64\n",
    "NUM_ARTISTS = 10 # this is 11 to get to 10 classes, can be changed..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Collect the top 10 artists, sort data\n",
    "\"\"\"\n",
    "\n",
    "# Collecting Needed Images\n",
    "artists = pd.read_csv(artist_csv_loc)\n",
    "\n",
    "# Creating a dataframe with the top 10 artists by number of paintings\n",
    "artists_sort = artists.sort_values(by=['paintings'], ascending=False)\n",
    "\n",
    "# add 1 to reach the first 10 classes\n",
    "artists_top = artists_sort.head(NUM_ARTISTS + 1)\n",
    "\n",
    "# Images\n",
    "artists_dir = os.listdir(images_dir) # Files are named after each artists\n",
    "\n",
    "# Images DataFrame\n",
    "artists_top_name = artists_top['name'].str.replace(' ', '_').values\n",
    "\n",
    "images_df = pd.DataFrame()\n",
    "for name in artists_top_name:\n",
    "    images_df = pd.concat([images_df, pd.DataFrame(data={'Path': glob.glob('data/images/images/' + name + '/*'), 'Name': name})], ignore_index=True)\n",
    "\n",
    "train_df = images_df.sample(frac=0.8, random_state=200)\n",
    "test_df = images_df.drop(train_df.index)\n",
    "\n",
    "if K.backend.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, IMG_HEIGHT, IMG_WIDTH)\n",
    "else:\n",
    "    input_shape = (IMG_HEIGHT, IMG_WIDTH, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Print total number of paintings for each artist\n",
    "\"\"\"\n",
    "\n",
    "# plot to print out the number of paintings per artist\n",
    "plt.figure(figsize=(20, 7)) # need to increase width to space out labels on bottom\n",
    "\n",
    "plt.title(\"Number of paintings by artist\")\n",
    "plt.ylabel(\"Number\")\n",
    "plt.xlabel(\"Artist\")\n",
    "\n",
    "artists_10 = artists_top.head(NUM_ARTISTS)\n",
    "\n",
    "# add labels onto the bar itself\n",
    "painting_num = list(artists_10['paintings'])\n",
    "for i in range(len(painting_num)): \n",
    "    plt.text(x=i, y=painting_num[i], s=painting_num[i], ha='center', va='bottom')\n",
    "\n",
    "# plot the chart\n",
    "plt.bar(artists_10['name'], artists_10['paintings'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Build generators\n",
    "\"\"\"\n",
    "\n",
    "train_generator = ImageDataGenerator(rescale=1.0 / 255,\n",
    "                                    rotation_range=20,\n",
    "                                    zoom_range=0.05,\n",
    "                                    width_shift_range=0.05,\n",
    "                                    height_shift_range=0.05,\n",
    "                                    shear_range=0.05,\n",
    "                                    horizontal_flip=True,\n",
    "                                    fill_mode=\"nearest\",\n",
    "                                    validation_split=0.15\n",
    "                                    )\n",
    "\n",
    "test_generator = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "train_gen = train_generator.flow_from_dataframe(\n",
    "    train_df,\n",
    "    shuffle=True,\n",
    "    x_col='Path',\n",
    "    y_col='Name',\n",
    "    class_mode='categorical',\n",
    "    subset=\"training\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "valid_gen = train_generator.flow_from_dataframe(\n",
    "    train_df,\n",
    "    subset=\"validation\",\n",
    "    shuffle=True,\n",
    "    x_col='Path',\n",
    "    y_col='Name',\n",
    "    class_mode='categorical',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "test_gen = test_generator.flow_from_dataframe(\n",
    "    test_df,\n",
    "    x_col='Path',\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    class_mode=None,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH)\n",
    ")\n",
    "\n",
    "# Set the amount of steps for training, validation, and testing data\n",
    "# based on the batch size\n",
    "steps_train = train_gen.n//train_gen.batch_size\n",
    "steps_valid = valid_gen.n//valid_gen.batch_size\n",
    "steps_test = test_gen.n//test_gen.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "The following model is a very simple convolutional neural network. This was the\n",
    "first model we tested, and used as a proof of concept early on in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hyperparameters\n",
    "\"\"\"\n",
    "\n",
    "N_EPOCHS = 3\n",
    "LEARNING_RATE = 0.0001 # 0.001 is the default for Adam set by TensorFlow\n",
    "OPTIMIZER = tf.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "LOSS_FUNCTION = tf.losses.CategoricalCrossentropy(from_logits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This block contains the code for the model itself\n",
    "\"\"\"\n",
    "\n",
    "base_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Rescaling(1./255),\n",
    "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(NUM_ARTISTS, activation='softmax')\n",
    "])\n",
    "\n",
    "# compile model\n",
    "base_model.compile(\n",
    "  optimizer=OPTIMIZER,\n",
    "  loss=LOSS_FUNCTION,\n",
    "  metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train the model\n",
    "\"\"\"\n",
    "\n",
    "# create a checkpoint for the model\n",
    "checkpt = ModelCheckpoint(filepath='baseline_model.hdf5', save_best_only=True, verbose=1)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, mode='min')\n",
    "\n",
    "# Fit the model\n",
    "history = base_model.fit_generator(\n",
    "    generator = train_gen,\n",
    "    steps_per_epoch=steps_train,\n",
    "    validation_data = valid_gen,\n",
    "    validation_steps = steps_valid,\n",
    "    verbose=1,\n",
    "    epochs=N_EPOCHS,\n",
    "    callbacks=[checkpt, early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create plots for validation accuracy and loss versus epochs\n",
    "\"\"\"\n",
    "\n",
    "# Create accuracy plot\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# create loss plot\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Make predictions for the model\n",
    "\"\"\"\n",
    "\n",
    "# Load model weights\n",
    "base_model.load_weights('baseline_model.hdf5')\n",
    "\n",
    "# make predictions using the test set\n",
    "predict = base_model.predict_generator(test_gen, steps=steps_test, verbose=1)\n",
    "\n",
    "# collect the actual index of the predicted class\n",
    "predicted_class = np.argmax(predict, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Produce result metrics\n",
    "\"\"\"\n",
    "\n",
    "# collect the actual classes of the predictions\n",
    "r = dict(train_gen.class_indices.items())\n",
    "y_true = test_df['Name']\n",
    "y_true = [r[k] for k in y_true]\n",
    "\n",
    "# create confusion matrix\n",
    "cm = confusion_matrix(y_true, predicted_class)\n",
    "sns.heatmap(cm, annot=True)\n",
    "\n",
    "# print classification report\n",
    "print(classification_report(y_true, predicted_class))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit (conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
